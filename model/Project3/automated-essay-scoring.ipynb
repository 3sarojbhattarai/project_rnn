{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.6/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sklearn) (0.21.3)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.18.1)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.4.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.14.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../input/automated-essay-scoring-dataset/\"\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10380 samples\n",
      "Epoch 1/50\n",
      "10380/10380 [==============================] - 8s 752us/sample - loss: 64.6568 - mae: 4.3912\n",
      "Epoch 2/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 40.8697 - mae: 3.6524\n",
      "Epoch 3/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 34.2867 - mae: 3.5350\n",
      "Epoch 4/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 31.7851 - mae: 3.4770\n",
      "Epoch 5/50\n",
      "10380/10380 [==============================] - 2s 177us/sample - loss: 29.8607 - mae: 3.3384\n",
      "Epoch 6/50\n",
      "10380/10380 [==============================] - 2s 172us/sample - loss: 27.9357 - mae: 3.1682\n",
      "Epoch 7/50\n",
      "10380/10380 [==============================] - 2s 204us/sample - loss: 26.0757 - mae: 2.9989\n",
      "Epoch 8/50\n",
      "10380/10380 [==============================] - 2s 183us/sample - loss: 23.4039 - mae: 2.7991\n",
      "Epoch 9/50\n",
      "10380/10380 [==============================] - 2s 168us/sample - loss: 20.8926 - mae: 2.6463\n",
      "Epoch 10/50\n",
      "10380/10380 [==============================] - 2s 185us/sample - loss: 18.1881 - mae: 2.4643\n",
      "Epoch 11/50\n",
      "10380/10380 [==============================] - 2s 172us/sample - loss: 16.9437 - mae: 2.3483\n",
      "Epoch 12/50\n",
      "10380/10380 [==============================] - 2s 167us/sample - loss: 16.1883 - mae: 2.2781\n",
      "Epoch 13/50\n",
      "10380/10380 [==============================] - 2s 169us/sample - loss: 14.8623 - mae: 2.2029\n",
      "Epoch 14/50\n",
      "10380/10380 [==============================] - 2s 166us/sample - loss: 14.3842 - mae: 2.1522\n",
      "Epoch 15/50\n",
      "10380/10380 [==============================] - 2s 171us/sample - loss: 13.4283 - mae: 2.0750\n",
      "Epoch 16/50\n",
      "10380/10380 [==============================] - 2s 181us/sample - loss: 13.0601 - mae: 2.0436\n",
      "Epoch 17/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 12.4658 - mae: 1.9837\n",
      "Epoch 18/50\n",
      "10380/10380 [==============================] - 2s 173us/sample - loss: 12.4113 - mae: 1.9611\n",
      "Epoch 19/50\n",
      "10380/10380 [==============================] - 2s 221us/sample - loss: 12.0714 - mae: 1.9405\n",
      "Epoch 20/50\n",
      "10380/10380 [==============================] - 2s 196us/sample - loss: 11.6188 - mae: 1.9178\n",
      "Epoch 21/50\n",
      "10380/10380 [==============================] - 2s 204us/sample - loss: 11.2332 - mae: 1.8607\n",
      "Epoch 22/50\n",
      "10380/10380 [==============================] - 2s 191us/sample - loss: 11.2957 - mae: 1.8648\n",
      "Epoch 23/50\n",
      "10380/10380 [==============================] - 2s 192us/sample - loss: 11.7205 - mae: 1.8882\n",
      "Epoch 24/50\n",
      "10380/10380 [==============================] - 2s 180us/sample - loss: 11.1853 - mae: 1.8431\n",
      "Epoch 25/50\n",
      "10380/10380 [==============================] - 2s 188us/sample - loss: 11.0077 - mae: 1.8234\n",
      "Epoch 26/50\n",
      "10380/10380 [==============================] - 2s 182us/sample - loss: 10.5726 - mae: 1.8062\n",
      "Epoch 27/50\n",
      "10380/10380 [==============================] - 2s 169us/sample - loss: 10.1855 - mae: 1.7771\n",
      "Epoch 28/50\n",
      "10380/10380 [==============================] - 2s 167us/sample - loss: 10.2099 - mae: 1.7620\n",
      "Epoch 29/50\n",
      "10380/10380 [==============================] - 2s 178us/sample - loss: 10.2155 - mae: 1.7758\n",
      "Epoch 30/50\n",
      "10380/10380 [==============================] - 2s 177us/sample - loss: 9.9699 - mae: 1.7504\n",
      "Epoch 31/50\n",
      "10380/10380 [==============================] - 2s 182us/sample - loss: 9.9003 - mae: 1.7444\n",
      "Epoch 32/50\n",
      "10380/10380 [==============================] - 2s 170us/sample - loss: 9.9623 - mae: 1.7373\n",
      "Epoch 33/50\n",
      "10380/10380 [==============================] - 2s 165us/sample - loss: 10.0367 - mae: 1.7468\n",
      "Epoch 34/50\n",
      "10380/10380 [==============================] - 2s 164us/sample - loss: 10.0345 - mae: 1.7380\n",
      "Epoch 35/50\n",
      "10380/10380 [==============================] - 2s 183us/sample - loss: 9.7887 - mae: 1.7217\n",
      "Epoch 36/50\n",
      "10380/10380 [==============================] - 2s 172us/sample - loss: 9.5706 - mae: 1.7156\n",
      "Epoch 37/50\n",
      "10380/10380 [==============================] - 2s 193us/sample - loss: 9.3003 - mae: 1.7010\n",
      "Epoch 38/50\n",
      "10380/10380 [==============================] - 2s 169us/sample - loss: 9.6716 - mae: 1.7089\n",
      "Epoch 39/50\n",
      "10380/10380 [==============================] - 2s 166us/sample - loss: 9.3183 - mae: 1.6854\n",
      "Epoch 40/50\n",
      "10380/10380 [==============================] - 2s 169us/sample - loss: 9.0465 - mae: 1.6669\n",
      "Epoch 41/50\n",
      "10380/10380 [==============================] - 2s 171us/sample - loss: 9.2162 - mae: 1.6768\n",
      "Epoch 42/50\n",
      "10380/10380 [==============================] - 2s 173us/sample - loss: 8.6223 - mae: 1.6482\n",
      "Epoch 43/50\n",
      "10380/10380 [==============================] - 2s 202us/sample - loss: 9.4176 - mae: 1.6845\n",
      "Epoch 44/50\n",
      "10380/10380 [==============================] - 2s 166us/sample - loss: 9.2687 - mae: 1.6718\n",
      "Epoch 45/50\n",
      "10380/10380 [==============================] - 2s 166us/sample - loss: 9.0173 - mae: 1.6567\n",
      "Epoch 46/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 8.7966 - mae: 1.6446\n",
      "Epoch 47/50\n",
      "10380/10380 [==============================] - 2s 178us/sample - loss: 8.5218 - mae: 1.6322\n",
      "Epoch 48/50\n",
      "10380/10380 [==============================] - 2s 180us/sample - loss: 8.8290 - mae: 1.6293\n",
      "Epoch 49/50\n",
      "10380/10380 [==============================] - 2s 179us/sample - loss: 8.4350 - mae: 1.6029\n",
      "Epoch 50/50\n",
      "10380/10380 [==============================] - 2s 175us/sample - loss: 8.6043 - mae: 1.6108\n",
      "Kappa Score: 0.9614815615648233\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 639us/sample - loss: 65.2163 - mae: 4.3689\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 2s 168us/sample - loss: 40.4392 - mae: 3.6041\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 2s 185us/sample - loss: 33.1707 - mae: 3.4462\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 2s 199us/sample - loss: 30.3118 - mae: 3.3492\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 28.9915 - mae: 3.2518\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 27.0305 - mae: 3.0509\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 24.4965 - mae: 2.8639\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 2s 188us/sample - loss: 20.2153 - mae: 2.5843\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 185us/sample - loss: 17.8081 - mae: 2.4097\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 16.7234 - mae: 2.3154\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 15.4415 - mae: 2.2388\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 14.6231 - mae: 2.1603\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 14.1300 - mae: 2.1147\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 13.9588 - mae: 2.1142\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 13.0238 - mae: 2.0389\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 13.0092 - mae: 2.0329\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 12.1463 - mae: 1.9694\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 12.0126 - mae: 1.9384\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 2s 167us/sample - loss: 11.4802 - mae: 1.9016\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 11.6916 - mae: 1.9086\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 11.2940 - mae: 1.8682\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 10.7905 - mae: 1.8315\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 10.9752 - mae: 1.8304\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 2s 194us/sample - loss: 10.5334 - mae: 1.7898\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 2s 219us/sample - loss: 10.3315 - mae: 1.7777\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 10.1392 - mae: 1.7815\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 2s 188us/sample - loss: 10.1229 - mae: 1.7603\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 9.7288 - mae: 1.7284\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 9.9006 - mae: 1.7390\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 2s 196us/sample - loss: 10.1044 - mae: 1.7690\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 9.8085 - mae: 1.7414\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 9.6859 - mae: 1.7390\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 9.6416 - mae: 1.7250\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 9.2558 - mae: 1.7128\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 9.4613 - mae: 1.7096\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 2s 198us/sample - loss: 9.3944 - mae: 1.7054\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 2s 193us/sample - loss: 9.6179 - mae: 1.7095\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 2s 188us/sample - loss: 9.1096 - mae: 1.6815\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 191us/sample - loss: 9.2098 - mae: 1.6871\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 8.8724 - mae: 1.6706\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 2s 185us/sample - loss: 9.0252 - mae: 1.6755\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 9.0782 - mae: 1.6813\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 8.6879 - mae: 1.6619\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 8.4383 - mae: 1.6428\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 8.9106 - mae: 1.6633\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 9.0992 - mae: 1.6669\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 8.8308 - mae: 1.6443\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 8.7345 - mae: 1.6513\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 2s 177us/sample - loss: 8.6572 - mae: 1.6531\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 8.4753 - mae: 1.6363\n",
      "Kappa Score: 0.95373067387431\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 687us/sample - loss: 64.6322 - mae: 4.4277\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 2s 197us/sample - loss: 41.8319 - mae: 3.6641\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 34.9628 - mae: 3.5045\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 31.6610 - mae: 3.4369\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 30.5332 - mae: 3.3331\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 28.8460 - mae: 3.1299\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 26.9637 - mae: 2.9959\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 22.9252 - mae: 2.7635\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 20.5539 - mae: 2.5977\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 18.2719 - mae: 2.4301\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 17.3636 - mae: 2.3586\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 170us/sample - loss: 16.8525 - mae: 2.3079\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 15.3435 - mae: 2.2219\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 2s 177us/sample - loss: 15.1302 - mae: 2.1982\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 14.7820 - mae: 2.1416\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 14.3707 - mae: 2.1199\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 13.8405 - mae: 2.0638\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 13.0251 - mae: 2.0174\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 2s 188us/sample - loss: 12.5830 - mae: 1.9995\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 2s 186us/sample - loss: 12.5190 - mae: 1.9553\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 12.4147 - mae: 1.9316\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 12.1182 - mae: 1.9215\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 11.4048 - mae: 1.8765\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 11.4458 - mae: 1.8585\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 11.2140 - mae: 1.8499\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 11.2280 - mae: 1.8379\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 11.5038 - mae: 1.8465\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 2s 185us/sample - loss: 10.8238 - mae: 1.8158\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 2s 170us/sample - loss: 10.3822 - mae: 1.7967\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 2s 223us/sample - loss: 10.4937 - mae: 1.7977\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 2s 197us/sample - loss: 10.3077 - mae: 1.7852\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 2s 216us/sample - loss: 10.3279 - mae: 1.7873\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 2s 177us/sample - loss: 9.8675 - mae: 1.7468\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 9.8688 - mae: 1.7614\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 2s 186us/sample - loss: 9.8433 - mae: 1.7639\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 2s 186us/sample - loss: 9.6515 - mae: 1.7331\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 2s 196us/sample - loss: 9.8208 - mae: 1.7414\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 9.4441 - mae: 1.7215\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 9.7233 - mae: 1.7199\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 186us/sample - loss: 9.3871 - mae: 1.7226\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 9.6359 - mae: 1.7370\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 9.1939 - mae: 1.7014\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 9.1498 - mae: 1.6910\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 9.1165 - mae: 1.6882\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 8.8209 - mae: 1.6654\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 8.9264 - mae: 1.6726\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 8.8323 - mae: 1.6615\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 9.1723 - mae: 1.6786\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 8.5105 - mae: 1.6459\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 8.9986 - mae: 1.6622\n",
      "Kappa Score: 0.9599516571868413\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 688us/sample - loss: 64.4187 - mae: 4.3449\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 2s 206us/sample - loss: 41.0541 - mae: 3.6632\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 2s 191us/sample - loss: 34.7704 - mae: 3.5244\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 2s 177us/sample - loss: 31.6908 - mae: 3.3976\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 2s 210us/sample - loss: 29.8929 - mae: 3.2658\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 28.0268 - mae: 3.1164\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 26.4810 - mae: 2.9602\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 2s 191us/sample - loss: 22.7746 - mae: 2.7249\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 19.7184 - mae: 2.5243\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 17.8853 - mae: 2.3967\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 16.2913 - mae: 2.2843\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 15.5291 - mae: 2.2175\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 14.4286 - mae: 2.1487\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 13.9794 - mae: 2.1043\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 13.2910 - mae: 2.0555\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 13.1434 - mae: 2.0379\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 2s 197us/sample - loss: 12.7100 - mae: 1.9779\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 12.4743 - mae: 1.9898\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 12.0907 - mae: 1.9297\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 12.2347 - mae: 1.9247\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 11.4484 - mae: 1.8877\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 11.4067 - mae: 1.8689\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 11.0956 - mae: 1.8318\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 10.8777 - mae: 1.8294\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 10.6585 - mae: 1.8057\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 2s 170us/sample - loss: 11.0391 - mae: 1.8135\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 10.4807 - mae: 1.7862\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 10.4445 - mae: 1.7858\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 2s 167us/sample - loss: 10.3458 - mae: 1.7714\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 10.1901 - mae: 1.7520\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 9.9942 - mae: 1.7434\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 9.7887 - mae: 1.7338\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 10.1769 - mae: 1.7546\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 2s 197us/sample - loss: 9.2868 - mae: 1.7041\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 2s 215us/sample - loss: 9.4694 - mae: 1.7192\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 2s 203us/sample - loss: 9.9308 - mae: 1.7282\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 2s 192us/sample - loss: 9.4863 - mae: 1.7112\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 9.5234 - mae: 1.7058\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 192us/sample - loss: 9.1959 - mae: 1.7074\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 9.3494 - mae: 1.7087\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 2s 186us/sample - loss: 8.8651 - mae: 1.6720\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 9.4114 - mae: 1.6988\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 9.2243 - mae: 1.6993\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 9.0261 - mae: 1.6643\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 192us/sample - loss: 8.7974 - mae: 1.6633\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 2s 191us/sample - loss: 8.9698 - mae: 1.6771\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 8.8783 - mae: 1.6563\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 9.0086 - mae: 1.6685\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 9.2267 - mae: 1.6705\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 8.4520 - mae: 1.6240\n",
      "Kappa Score: 0.9613986558207013\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10381 samples\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 637us/sample - loss: 61.9723 - mae: 4.2830\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 38.3076 - mae: 3.5125\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 2s 168us/sample - loss: 32.9615 - mae: 3.4304\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 29.8119 - mae: 3.3187\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 28.0781 - mae: 3.1956\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 2s 175us/sample - loss: 25.9411 - mae: 3.0146\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 2s 168us/sample - loss: 24.6908 - mae: 2.8800\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 2s 232us/sample - loss: 21.8657 - mae: 2.6751\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 2s 184us/sample - loss: 18.3429 - mae: 2.4639\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 2s 194us/sample - loss: 16.9238 - mae: 2.3526\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 2s 190us/sample - loss: 15.6742 - mae: 2.2437\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 14.8288 - mae: 2.1711\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 14.2790 - mae: 2.1442\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 13.7725 - mae: 2.0960\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 2s 192us/sample - loss: 13.8150 - mae: 2.0758\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 12.7702 - mae: 2.0175\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 12.0509 - mae: 1.9609\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 2s 191us/sample - loss: 11.5481 - mae: 1.9324\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 11.8058 - mae: 1.9200\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 11.1959 - mae: 1.8764\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 11.1405 - mae: 1.8586\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 2s 172us/sample - loss: 10.9290 - mae: 1.8512\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 2s 178us/sample - loss: 10.2747 - mae: 1.8095\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 10.5254 - mae: 1.8015\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 2s 180us/sample - loss: 10.4282 - mae: 1.7826\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 9.7692 - mae: 1.7614\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 2s 168us/sample - loss: 9.7930 - mae: 1.7451\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 9.6727 - mae: 1.7539\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 2s 169us/sample - loss: 9.7150 - mae: 1.7455\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 2s 168us/sample - loss: 9.8553 - mae: 1.7547\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 2s 183us/sample - loss: 9.5514 - mae: 1.7378\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 2s 189us/sample - loss: 9.1558 - mae: 1.7100\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 2s 173us/sample - loss: 9.0650 - mae: 1.7042\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 2s 171us/sample - loss: 9.0151 - mae: 1.6887\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 8.8017 - mae: 1.6803\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 9.1061 - mae: 1.6921\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 2s 177us/sample - loss: 8.9144 - mae: 1.6819\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 2s 165us/sample - loss: 8.8283 - mae: 1.6785\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 2s 164us/sample - loss: 8.7833 - mae: 1.6592\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 2s 226us/sample - loss: 8.5060 - mae: 1.6448\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 2s 179us/sample - loss: 8.6542 - mae: 1.6634\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 2s 218us/sample - loss: 8.5472 - mae: 1.6495\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 8.2558 - mae: 1.6164\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 2s 185us/sample - loss: 8.4480 - mae: 1.6395\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 2s 174us/sample - loss: 8.4759 - mae: 1.6458\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 2s 176us/sample - loss: 7.8782 - mae: 1.5911\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 2s 187us/sample - loss: 8.2614 - mae: 1.6122\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 8.2059 - mae: 1.6100\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 2s 182us/sample - loss: 7.8825 - mae: 1.6051\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 2s 181us/sample - loss: 8.3023 - mae: 1.6214\n",
      "Kappa Score: 0.9578640870725509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    \n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "    \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    \n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./final_lstm.h5')\n",
    "            \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9589\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e27eed44fbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./word2vecmodel.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#model = Word2vec.KeyedVectors.load_word2vec_format(os.path.join(current_path, \"./word2vec.bin\"), binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclean_test_essays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "    from gensim.test.utils import datapath\n",
    "    \n",
    "    content = \"He is very bad student.\"    \n",
    "    \n",
    "    if len(content) > 20:\n",
    "        num_features = 300\n",
    "        model = Word2Vec.load('./word2vecmodel.bin')\n",
    "        #model = Word2vec.KeyedVectors.load_word2vec_format(os.path.join(current_path, \"./word2vec.bin\"), binary=True)\n",
    "        clean_test_essays = []\n",
    "        clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
    "        testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "        testDataVecs = np.array(testDataVecs)\n",
    "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "        lstm_model = get_model()\n",
    "        lstm_model.load_weights(os.path.join(current_path, \"./final_lstm.h5\"))\n",
    "        preds = lstm_model.predict(testDataVecs)\n",
    "\n",
    "        if math.isnan(preds):\n",
    "            preds = 0\n",
    "        else:\n",
    "            preds = np.around(preds)\n",
    "\n",
    "        if preds < 0:\n",
    "            preds = 0\n",
    "        if preds > question.max_score:\n",
    "            preds = question.max_score\n",
    "    else:\n",
    "        preds = 0\n",
    "        \n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
